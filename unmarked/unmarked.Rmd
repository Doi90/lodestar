---
title: "Intro to unmarked"
author: "David Wilkinson"
date: "13 October 2017"
output: 
  html_document:
    css: lodestar.css
    toc: yes
    toc_float:
      collapsed: no
      toc_depth: 4
---

<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #208000;
}
</style>

```{r echo = FALSE, message = FALSE}
library(knitr)
library(kableExtra)
library(lattice)
```

<hr class="double">

## Occupancy Modelling with Imperfect Detection

Site occupancy surveys are frequently used in the monitoring of species. Site occupancy probabilities can be used as a metric when monitoring the current state of a population. This could be for overall changes in occupancy or the expansion/contraction of species distributions. The premise being that changes in site occupancy are correlated with changes in population size and characteristics.

However, field surveys face the problem of imperfect detection; there is no guarantee that each individual, or even each species, will be detected at a site even if it is present. The detectability of a species can be defined as the probability that at least one individual is detected in a particular sampling effort, conditional to the species being present in the area of interest during sampling. To be able to estimate the effect of imperfect detection we need to perform several *independent* surveys of the same site.

<hr class="small">

### The Maths

A single species, single season occupancy model like we will cover today can be seen as a hierarchical model with two processes:

  + The *state* process: defined by the *true* site occupancy status (site occupied by the species or not)
  + The *observation* process: given the occupancy status, was the species detected
  
The *true* occupancy status of a site, which is unknown, is represented by the latent variable $z_i$, which takes the value $1$ if site $i$ is occupied, or $0$ otherwise, and is modelled as:

$$z_i \sim Bernoulli(\psi_i)$$

where $\psi_i$ is the probability of occupancy. The observation process is modelled as:

$$y_{ij} | z_i \sim Bernoulli(z_i * p_{ij})$$

where $y_{ij}$ is our detection data site $i$ and sampling repeat $j$, and $p_{ij}$ is the probability of detecting the species given it is present.

As written, these equations effectively assume the probabilities of occupancy and detection are constant across all sites, when in reality sites are heterogeneous. Covariates can be incorporated into the model using a logit-link function (like in generalised linear regression), such that the occupancy and/or detectability at site $i$ can be modelled as a function of a series of covariates $\{x_{i1}, ..., x_{ik}\}$.

$$logit(\psi_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} ... \beta_kx_{ik}$$
$$logit(p_i) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} ... \beta_kx_{ik}$$

The four main assumptions of this model are:

  + A site is closed to changes in occupancy over the survey season
  + The probability of occupancy is constant across all sites, or the variation is modelled by covariates
  + The probability of detection is constant across all sites and surveys, or the variation is modelled by covariates
  + The detection of a species and its detection histories at each site are independent

`unmarked` is an R package developed by Ian Fiske and Richard Chandler (and contributed to by Marc KÃ©ry, Andy Royle, David Miller amongst others) to help users fit models accounting for the imperfect detection of *unmarked* individuals.

<hr class="big">

## unmarked Models

`unmarked` is able to fit several different models under this banner. This ranges from standard occupancy models to distance sampling and colonisation extinction models. While this guide will only cover `occu()` for standard occupancy models, only minimal adjustment is required to fit the others

```{r echo=FALSE}
a <- data.frame(Model = c("Occupancy",
                          "Royle-Nichols",
                          "False Positives",
                          "Point Count",
                          "Generalised Point Count",
                          "Open Point Count",
                          "Distance Sampling",
                          "Generalized Distance Sampling",
                          "Arbitrary multinomial Poisson",
                          "Colinization Extinction",
                          "Generalized multinomial mixture"),
           FittingFunction = c("occu", "occuRN", "occuFP", "pcount", "gpcount",
                               "pcountOpen", "distsamp", "gdistsamp", "multinomPois",
                               "colext", "gmultmix"),
           Data = c("unmarkedFrameOccu", "unmarkedFrameOccu", "unmarkedFrameOccuFP",
                    "unmarkedFramePCount", "unmarkedFrameGPC", "unmarkedFramePCO",
                    "unmarkedFrameDS", "unmarkedFrameGDS", "unmarkedFrameMPois",
                    "unmarkedMultFrame", "unmarkedFrameGMM"))

kable(a)
```

<hr class="big">

## Load Package

```{r warning=FALSE, message=FALSE}
library(unmarked)
```

<hr class="big">

## Load Data

For this guide we will use a sample dataset supplied with the package.

```{r}
data <- read.csv(system.file("csv", "widewt.csv", package = "unmarked"))
```

```{r echo = FALSE}
kable(data, "html") %>%
  kable_styling() %>%
  scroll_box(width = "800px", height = "400px")
```

Data needs to be formatted for use with a specific model fitting function, and this is accomplished with a call to an appropriate `unmarkedFrame`. `unmarkedFrame`'s are a special class of object used in this package for organising our data. These are built using `R`'s `S4` reference class system instead of the more common `S3` (like a linear model object `lm`). The benefit of this is a formal definition and validity checking to prevent accidentally breaking it, but the consequence is a slightly different syntax for examining it (covered later). In our case, to fit an `occu()` model we need to use the `unmarkedFrameOccu()`. This requires two `data.frame`s and a `list` of `data.frame`s: 

  + `y`: A `data.frame` of presence-absence records. Rows are sites, columns are repeat visits.
  + `siteCovs`: A `data.frame` of the site-level covariates. These are things that *don't* change between visits like elevation, annual rainfall, distance from roads, etc.. One column per covariate
  + `obsCovs`: A `list` of `data.frame`s for the observation-level covariates. Each covariate is its own `data.frame` with rows as sites and columns as repeat visits. These are things that *can* change between visits. Could be environmental conditions like daily temperature or cloud cover, or methodological variables like survey method (spotlighting, pitfall traps, ink cards etc.) or an observer ID.

```{r warning=FALSE, message=FALSE}
y <- data[ ,2:4]

siteCovs <-  data[ ,5:7]

obsCovs <- list(date = data[ ,8:10],
                ivel = data[ ,11:13])

umf <- unmarkedFrameOccu(y = y, siteCovs = siteCovs, obsCovs = obsCovs)

summary(umf)
```

<hr class="big">

## Standardising Data

If you haven't done this before creating the `unmarkedFrame` object the syntax for subsetting changes slightly since an `unmarkedFrame` is an S4 class. The first `$` subset is replaced with an `@`

```{r warning=FALSE, message=FALSE}
umf@siteCovs$elev <- scale(umf@siteCovs$elev)
umf@siteCovs$forest <- scale(umf@siteCovs$forest)
umf@siteCovs$length <- scale(umf@siteCovs$length)

umf@obsCovs$date <- scale(umf@obsCovs$date)
umf@obsCovs$ivel <- scale(umf@obsCovs$ivel)
```

<hr class="big">

## Missing Data Considerations

We have two types of data in these models: our observation data and our covariate data. These models account for missing data in the two groups differently. 

  + You *are* allowed to have missing records in your observation data. In practice this could result from varying numbers of repeat visits to a site or the loss of data. As long as a site has at least one observation it can be used in the model. Any site that has zero observations will be removed.
  + You *cannot* have missing covariate data. If a site is missing the data for a site-level covariate it will be removed before model fitting. If a site is missing observation-level covariate data it will only be removed if it is missing for *all* observations, but the observation with missing data is removed. This is *very* important to consider when performing model selection so we will touch on this again later.

<hr class="big">

## Fitting a Model

Fitting a model with `unmarked` uses a similar syntax to fitting a linear model with the `lm()` function. The main difference is that the way a formula is defined varies. In the case of `occu()` it requires a double right-hand side formula for detection and occupancy covariates in that order. Essentially `~ detection formula ~ occupancy formula`.

```{r warning=FALSE, message=FALSE}
fm <- occu(formula = ~ 1 
                     ~ 1,
           data = umf)

fm
```

`unmarked` estimates are on the link-scale (logit for `occu()` since it uses a logit-link), and the `backTransform()` function coverts them back to the original scale. You need to specify a type of `state` or `det` for occupancy or detection covariates. If you have fit a model with covariates then you need to specify values for them (i.e. what is the probability of occupancy when CovA = X and CovB = Y)

```{r warning=FALSE, message=FALSE}
backTransform(fm, type = "state")
```

```{r warning=FALSE, message=FALSE}
backTransform(fm, type = "det")
```

Alternatively, you can define an antilogit function:

```{r warning=FALSE, message=FALSE}
antilogit <- function(x) { exp(x) / (1 + exp(x) ) }
```

```{r warning=FALSE, message=FALSE}
antilogit(-0.665)
```

```{r warning=FALSE, message=FALSE}
antilogit(1.32)
```

Now, to add some covariates. First, assuming constant detection:

```{r warning=FALSE, message=FALSE}
fm1 <- occu(formula = ~ 1 
                      ~ forest + elev + length,
            data = umf)

fm1
```

And some more:

```{r warning=FALSE, message=FALSE}
fm2 <- occu(formula = ~ date + ivel + forest 
                      ~ forest + elev + length,
            data = umf)
fm2
```

<hr class="big">

## Model Selection

### unmarked

The is an in-built model selection method using `fitList()` and `modSel()`. You can name the models whatever you like, but the convention used here is common online (even if awkward to type out, especially for large numbers of covariates).

```{r}
fit <- fitList('psi(.)p(.)' = fm,
               'psi(forest + elev + length)p(.)' = fm1,
               'psi(forest + elev + length)p(date + ivel + forest)' = fm2)

modSel(fit)
```

<hr class="small">

### MuMIn

All `unmarked` models are compatible with the functions in the `MuMIn` package. For example, the `dredge()` and `pdredge()` functions perform dredge-based model selection that takes a 'full' or 'global' model and fits every possible combination of covariates and ranks them by some information criteria (AIC, BIC, etc.).

```{r echo=FALSE}
library(MuMIn)
```

```{r}
full <- occu(formula = ~ date + ivel + forest 
                       ~ forest + elev + length,
             data = umf)

modelList <- dredge(full,
                    rank = "AIC")
```

```{r echo = FALSE}
kable(modelList, "html") %>%
  kable_styling() %>%
  scroll_box(width = "800px", height = "400px")
```

<hr class="small">

### Missing Data

As mentioned previously, missing data can have serious impacts on model selection. This is because information criterion like AIC **are not** comparable between models fit to different datasets. If you fit *Model A* with covariates 1 & 2 with 100 sites, and *Model B* with covariates 1, 2, & 3 with 90 sites (because covariate 3 is missing data) then you cannot compare AIC. This rule holds no matter how minuscule the difference is (e.g. 10,000 vs 9,999).

To account for this you need to manually remove the data for all sites/observations that are missing data for any covariates that could potentially be included in your model. In the previous example, both *Model A* and *Model B* should be fit to the 90 sites not missing data for covariate 3.

<hr class="big">

## Proportion of Area Occupied

Imperfect detection can bias our occupancy estimates. So how do we calculate the actual proportion of area occupied by our target species while accounting for imperfect detection?

<hr class="small">

### Assuming perfect detection

If we naively assumed perfect detection the proportion of area occupied by a species is simply the proportion of occupied sites: number of sites where the species was observed divided by the total number of sites.

```{r}
siteValue <- apply(X = data[,2:4],
                   MARGIN = 1,
                   FUN = "max", na.rm = TRUE)

mean(siteValue)
```

<hr class="small">

### Accounting for Imperfect Detection

In reality we wouldn't assume perfect detection, so we need to account for it otherwise we will end up with a biased *underestimate* of the proportion of area occupied. `unmarked` utilises empirical Bayes methods to estimate the posterior distributions of the random variables (here, latent occurrence state, $z_i$).

$$Pr(z_i = 1 | y_{ij}, \psi_i, p_{ij})$$

```{r}
AICbest <- occu(formula = ~ forest + ivel
                          ~ elev,
                data = umf)

re <- ranef(AICbest)
EBUP <- bup(re, stat="mean")
CI <- confint(re, level=0.9)
rbind(PAO = c(Estimate = sum(EBUP), colSums(CI)) / 237)
```

A difference in the proportion of area occupied of over 15%! Depending on how hard a species can be to detect with a given survey method this difference can potentially be much higher!

<hr class="big">

## Prediction

Lets load in a bigger dataset for this one. This is a dataset for ink card surveys for the giant gecko, *Cyrtodactylus sadleiri*, on Christmas Island.

```{r}
gg <- read.csv("GG.csv")

umfGG <- unmarkedFrameOccu(y = gg[ ,2:11],
                           siteCovs = gg[ ,52:55],
                           obsCovs = list(BHG = gg[ ,12:21],
                                          Rat = gg[ ,22:31],
                                          Height = gg[ ,32:41],
                                          Orientation = gg[ ,42:51]))
```

Now lets fit a model with a larger amount of covariates. We can also alter some additional model parameters. `method` sets to optimisation method used by `optim` under the hood to estimate the maximum likelihood, `control` here sets the maximum number of iterations allowed for the model to converge (the default value is rather small so this is useful for more complex models), and `engine` gives you the choice to run the optimisation in native `R` code or fast `C++`.

```{r}
occuGG <- occu(formula = ~ BHG + Rat + Height + Orientation + 
                           DistRoad.S + Slope.S + I(Slope.S^2)
                         ~ YCASC + CHM.S + DistRoad.S,
               data = umfGG,
               method = "BFGS",
               control = list(maxit = 10000),
               engine = "C")
```

Now we load in the data we want to predict occupancy for. This dataset needs to have two columns for longitude and latitude values for each site/pixel, and a column for each covariate (each row is one site/pixel). Categorical variables need to have a single level specified per site. This is straight forward for something like vegetation class, but for methodological variables like survey method(ink cards, spotlighting, etc.) you need to select one *a priori*. 

```{r}
ggPred <- read.csv("SiteValues.csv")
```

And predict.

```{r}
occuPred <- predict(occuGG,
                    type = "state",
                    newdata = ggPred,
                    na.rm = TRUE,
                    inf.rm = TRUE)

levelplot(Predicted ~ ggPred$x + ggPred$y,
          data = occuPred,
          col.regions = rev(terrain.colors(100)),
          at = seq(0,1,length.out=101))
```

<hr class="big">

## Model Evaluation

A vital step in modelling is evaluating the accuracy of model predictions. Model evaluation usually involves testing how well our model can predict data *not* used in the model fitting process. Does our model predict high probability of occupancy for sites with observed presences in our hold-out data? Does it predict low probability of ocurrence for the sites with observed absences? The Area Under the Receiver Operating Curve (AUC) is a popular metric for this evaluation.

The problem with this type of model evaluation for models accounting for imperfect detection is that we don't know the *true* occupancy state of a site. Is the species actually absent from the site in our hold-out data, or did we just fail to detect it? As such this type of model evaluation is not appropriate for our modelling method.

Instead we can use several goodness-of-fit tests to perform model evaluation. Here we use three tests: sum of squared errors, Pearson's Chi-squared, and Freeman-Tukey Chi-squared. We create a new function called `fitstats` (where *model.name* should be replaced with your own), and then use it when fitting our model to bootsrapped samples of our data to assess model fit. Even though these are called goodness-of-fit tests they are looking for significant lack-of-fit evidence (which sounds too pessimistic), so a p-value of <0.05 is actually a poorly fit model. A "good" model should return a value >> 0.05 for each test. Yet the issue of imperfect detection can still impact these tests. It is possible to fail one test but pass the others on the basis of poor prediction at a small subset of sites (even one). For example, an observed presence at a really low probability of detection site can dramatically inflate the test statistic. In these scenarios it is important to assess the input of each site to the test statistic to determine if the "lack-of-fit" can be disregarded as over-penalising the mismatch in a very low proportion of sites. 

```{r eval = FALSE}
fitstats <- function(model.name, method = "nonparboot") {
  observed <- getY(model.name@data)
  expected <- fitted(model.name)
  resids <- residuals(model.name, method = "nonparboot")
  sse <- sum(resids^2, na.rm=TRUE)
  chisq <- sum((observed - expected)^2 / expected, na.rm=TRUE)
  freeTuke <- sum((sqrt(observed) - sqrt(expected))^2, na.rm=TRUE)
  out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)
  return(out)
}

pb <- parboot(model.name, fitstats, nsim=1000, report=TRUE, method = "nonparboot")
pb
par(mfrow=c(3,1))
plot(pb, main="", xlab=c("SSE","Chisq","FT"))
```

<hr class="big">

## Common Errors

  + <span style="color:red">Error: Hessian is singular. Try providing starting values or using fewer covariates.</span> (The bane of my Masters existence)
    + This could come about for a variety of reasons.
    + For models with a very low naive occupancy rate, especially in small datasets, you might need to specify starting values for the maximum likelihood estimation (see `?occu`)
    + You have complete separation in one or more covariates. This means the model doesn't have the necessary data to calculate parameter estimates. For categorical variables this is commonly a factor level that has exclusively presence *or* absence observations (or potentially when the prevalence is near 0 or 1 in very big datasets), or there are factor levels in two or more categorical variables that never co-occur. Consider merging factor levels if possible! For continuous variables this is where the data is split into two (or more) groups at either end of a sampling range. For example, you are including elevation as a covariate but have presence records only at 0-20m, and absences only at 80-100m. The model then can't estimate the slope of the relationship.
    + You have somehow got a continuous variable set to  factor
    + You have too many covariates in the model for your data. There is no set rule for this and different people will give different answers. Every 10 sites allows one covariate, every 10 presences allows one covariate, every 10 allows one covariate in each formula, each level in a categorical variable is treated as a separate binary dummy variable and it does/does not count as a separate covariate for this count etc..
  + <span style="color:red">Something about factor levels and prediction.</span>
    + If you have a model with a categorical covariate with 5 factor levels, the data being predicted to must have 5 levels. Each site being predicted to can have the same level, but the data must be allowed to have other levels. This is an `R` error and not a statistical error. This is commonly caused by methodological observation-level covariates. For example, your data comes from three different survey methods (e.g. spotlighting, trapping, and drones), you fit the model and find out that spotlighting has the best probability of detecting your target species, and then predict to only spotlighting as a survey method. A really easy mistake to make!
    + A dodgy fix: Set a random cell on the border of the area you are predicting to to the missing factor level
    + A proper fix: Use the `levels()` function to set additional levels to your prediction data
  + <span style="color:red">Your fitted model has unrealistic parameter estimates.</span>
    + `occu()` is on the logit-scale, so anything outside [-5, 5] is when you need to start checking. When this is back transformed to the probability scale it is roughly [0, 1]. There is leeway if you are considering polynomial terms. If you start getting parameter estimates of +30, -90, or +462.10 (David's high score) then you definitely have an issue! This is equally likely to happen to categorical or continuous data.
    + This could be one of two issues:
        + Your model has not converged on the maximum likelihood. Use the `control` argument when fitting the model to increase the number of iterations in the optimisation process. The default is rather small at 100 or 500 depending on your optimisation method. Setting it high (i.e. 10,000) is fine, as it is a maximum, and the model will stop if it converges earlier.
        + Your model has converged on a local maximum. Likelihoods can have multiple peaks, but only one maximum, and your model is stuck on the wrong one. To fix this use the `starts` argument to provide starting values for parameters to start the optimisation in a different area.
  +  <span style="color:red">Model selection with `dredge()` is too much for my computer to handle.</span>
    + See David (for Spartan) or Casey/Nick (for Boab)

<hr class="big">

## Additional resources

  + [*Overview of unmarked*](https://cran.r-project.org/web/packages/unmarked/vignettes/unmarked.pdf) vignette by Ian Fiske and Richard Chandler. The official intro guide that I copied some of the early examples from.
  + [*unmarked: An R Package for Fitting Hierarchical Models of Wildlife Occurrence and Abundance*](https://www.google.com.au/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&ved=0ahUKEwiFws-PtojXAhXCXrwKHdxKA70QFghLMAQ&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv043i10%2Fv43i10.pdf&usg=AOvVaw25YLdQ8rX4TzNWw0NGUneI). The paper about the package
  + The `unmarked` [website](https://sites.google.com/site/unmarkedinfo/home).
  + The `unmarked` [google group/forum](https://groups.google.com/forum/#!forum/unmarked). This is frequented by most of the package creators/contributors and other occupancy modellers. More often than not the answer to your problems has already been solved here.
  
<hr class="double">  